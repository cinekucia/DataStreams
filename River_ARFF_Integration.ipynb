{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJQtT72tW8TH",
        "outputId": "fb5b1838-16ed-4d6e-be81-66c3a6d52452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting river\n",
            "  Downloading river-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from river) (2.0.2)\n",
            "Collecting pandas<3.0.0,>=2.2.3 (from river)\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m883.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from river) (1.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->river) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=2.2.3->river) (1.17.0)\n",
            "Downloading river-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, river\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-2.2.3 river-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "from river.datasets import synth\n",
        "\n",
        "def save_stream_to_arff(stream, relation_name=\"agrawal_stream\", output_file=\"agrawal_stream.arff\"):\n",
        "    stream = list(stream)  # Unpack the stream to inspect\n",
        "    if not stream:\n",
        "        raise ValueError(\"Stream is empty\")\n",
        "\n",
        "    x_example, _ = stream[0]\n",
        "    feature_names = list(x_example.keys())\n",
        "    class_labels = sorted(set(y for _, y in stream))\n",
        "\n",
        "    with open(output_file, \"w\") as f:\n",
        "        # Header\n",
        "        f.write(f\"@RELATION {relation_name}\\n\\n\")\n",
        "\n",
        "        for feat in feature_names:\n",
        "            f.write(f\"@ATTRIBUTE {feat} NUMERIC\\n\")  # You can customize this if needed\n",
        "\n",
        "        f.write(f\"@ATTRIBUTE class {{{', '.join(map(str, class_labels))}}}\\n\\n\")\n",
        "        f.write(\"@DATA\\n\")\n",
        "\n",
        "        # Data rows\n",
        "        for x, y in stream:\n",
        "            values = [str(x[feat]) for feat in feature_names]\n",
        "            values.append(str(y))\n",
        "            f.write(\",\".join(values) + \"\\n\")\n",
        "\n",
        "    print(f\"Stream saved to {output_file} in ARFF format.\")\n",
        "\n",
        "# Usage\n",
        "stream = deepcopy(synth.Agrawal()).take(1000)\n",
        "save_stream_to_arff(stream)\n"
      ],
      "metadata": {
        "id": "h3yxRy3EXSD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from river import evaluate, metrics, tree, ensemble, forest\n",
        "from river.datasets import synth\n",
        "from copy import deepcopy\n",
        "\n",
        "# -------------------------------\n",
        "# Define Stream Generators\n",
        "# -------------------------------\n",
        "binary_classification_datasets = [\n",
        "    synth.Agrawal()\n",
        "]\n",
        "\n",
        "all_datasets = {\n",
        "    ds.__class__.__name__: ds for ds in binary_classification_datasets\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluation Loop\n",
        "# -------------------------------\n",
        "for dataset_name, stream in all_datasets.items():\n",
        "    print(f\"\\nğŸ” Dataset: {dataset_name}\")\n",
        "\n",
        "    # Re-initialize stream (take 1999 samples)\n",
        "    dataset = deepcopy(stream).take(1999)\n",
        "\n",
        "    # Define base model\n",
        "    base_model = tree.HoeffdingTreeClassifier(grace_period=50, delta=0.01)\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        \"Static SRPClassifier\": ensemble.SRPClassifier(\n",
        "            model=base_model.clone(), n_models=10, seed=42\n",
        "        ),\n",
        "        \"ARFClassifier\": forest.ARFClassifier(\n",
        "            seed=8, leaf_prediction=\"mc\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        dataset = deepcopy(stream).take(1999)  # Reset stream again\n",
        "\n",
        "        # Define metrics\n",
        "        acc = metrics.Accuracy()\n",
        "        prec = metrics.Precision()\n",
        "        rec = metrics.Recall()\n",
        "        kappa = metrics.CohenKappa()\n",
        "\n",
        "        # Track sample count and early debug\n",
        "        count = 0\n",
        "\n",
        "        for x, y in dataset:\n",
        "            y_pred = model.predict_one(x)\n",
        "\n",
        "            # Only update metrics if we got a prediction\n",
        "            if y_pred is not None:\n",
        "                acc.update(y, y_pred)\n",
        "                prec.update(y, y_pred)\n",
        "                rec.update(y, y_pred)\n",
        "                kappa.update(y, y_pred)\n",
        "\n",
        "            model.learn_one(x, y)\n",
        "            count += 1\n",
        "\n",
        "            if count == 10:  # Show early predictions to debug\n",
        "                print(f\"ğŸ§ª Sample {count}: True={y}, Pred={y_pred}\")\n",
        "\n",
        "        # Output results\n",
        "        print(f\"{model_name:>25}:\")\n",
        "        for m in [acc, prec, rec, kappa]:\n",
        "            print(f\"  {m.__class__.__name__:>15}: {m.get():.4f}\")\n"
      ],
      "metadata": {
        "id": "vlKUmWLdXEws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arff_dataset.py\n",
        "\n",
        "import pathlib\n",
        "from typing import Any, Dict, Iterator, List, Tuple, Callable, Optional\n",
        "from scipy.io import arff\n",
        "from river import datasets\n",
        "\n",
        "class ARFFDataset(datasets.base.FileDataset):\n",
        "    \"\"\"A streaming view over a local ARFF file.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    filename\n",
        "        Name of the .arff file (must be in `directory` or as given by a full path).\n",
        "    directory\n",
        "        Directory where to find `filename`. If None, `filename` may be an absolute path.\n",
        "    target\n",
        "        Name of the attribute to use as the target.\n",
        "    task\n",
        "        One of Riverâ€™s task constants (e.g. BINARY_CLF, MULTI_CLF, REG).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        filename: str,\n",
        "        directory: str | None = None,\n",
        "        *,\n",
        "        target: str,\n",
        "        task: str,\n",
        "    ):\n",
        "        # Resolve the full path\n",
        "        path = pathlib.Path(directory or \"\") / filename\n",
        "\n",
        "        # Load once to get both data array and metadata\n",
        "        with open(path, \"r\") as f:\n",
        "            data_arr, meta = arff.loadarff(f)\n",
        "\n",
        "        # Build attribute metadata list: (name, type_name, domain)\n",
        "        attrs_meta: List[Tuple[str, str, Optional[List[str]]]] = []\n",
        "        for name in meta.names():\n",
        "            attr = meta._attributes[name]\n",
        "            if attr.type_name == \"nominal\":\n",
        "                domain = [v.decode('utf-8') if isinstance(v, bytes) else str(v)\n",
        "                          for v in attr.values]\n",
        "                attrs_meta.append((name, \"nominal\", domain))\n",
        "            else:\n",
        "                attrs_meta.append((name, attr.type_name, None))\n",
        "\n",
        "        # Identify feature names and target meta\n",
        "        feature_names = [n for n, _, _ in attrs_meta if n != target]\n",
        "        target_meta = next((m for m in attrs_meta if m[0] == target), None)\n",
        "        if target_meta is None:\n",
        "            raise ValueError(f\"Target attribute '{target}' not found in ARFF header.\")\n",
        "\n",
        "        # Converter for target values: if numeric domain strings, cast to int; else keep as str or float\n",
        "        _, t_type, t_domain = target_meta\n",
        "        self._target_converter: Callable[[Any], Any]\n",
        "        if t_type == \"nominal\" and t_domain is not None:\n",
        "            # Try integer conversion\n",
        "            try:\n",
        "                int_vals = [int(v) for v in t_domain]\n",
        "                self._target_converter = lambda v: int(v)\n",
        "            except ValueError:\n",
        "                # Fallback: use string\n",
        "                self._target_converter = lambda v: v\n",
        "        elif t_type == \"numeric\":\n",
        "            self._target_converter = lambda v: float(v)\n",
        "        else:\n",
        "            self._target_converter = lambda v: v\n",
        "\n",
        "        # Infer counts\n",
        "        n_features = len(feature_names)\n",
        "        n_samples = data_arr.shape[0]\n",
        "        n_classes = None\n",
        "        if task in (datasets.base.BINARY_CLF, datasets.base.MULTI_CLF) and t_domain is not None:\n",
        "            n_classes = len(t_domain)\n",
        "\n",
        "        # Initialize base FileDataset\n",
        "        super().__init__(\n",
        "            filename=filename,\n",
        "            directory=directory,\n",
        "            task=task,\n",
        "            n_features=n_features,\n",
        "            n_classes=n_classes,\n",
        "            n_samples=n_samples,\n",
        "        )\n",
        "\n",
        "        # Persist for iteration\n",
        "        self._attrs_meta = attrs_meta\n",
        "        self._data_arr = data_arr\n",
        "        self._target = target\n",
        "\n",
        "    def _iter(self) -> Iterator[Tuple[Dict[str, Any], Any]]:\n",
        "        for rec in self._data_arr:\n",
        "            x: Dict[str, Any] = {}\n",
        "            y: Any = None\n",
        "            for name, typ, domain in self._attrs_meta:\n",
        "                val = rec[name]\n",
        "                # Bytes â†’ str\n",
        "                if isinstance(val, bytes):\n",
        "                    val = val.decode(\"utf-8\")\n",
        "                # Convert target\n",
        "                if name == self._target:\n",
        "                    try:\n",
        "                        y = self._target_converter(val)\n",
        "                    except Exception:\n",
        "                        y = val\n",
        "                else:\n",
        "                    # Feature: keep numeric types as float, others as str\n",
        "                    if typ == \"numeric\":\n",
        "                        x[name] = float(val)\n",
        "                    else:\n",
        "                        x[name] = val\n",
        "            yield x, y\n",
        "\n",
        "    def __iter__(self) -> Iterator[Tuple[Dict[str, Any], Any]]:\n",
        "        # Ensure file exists, then create a fresh iterator\n",
        "        path = pathlib.Path(self.directory or \"\") / self.filename\n",
        "        if not path.exists():\n",
        "            raise FileNotFoundError(f\"ARFF file not found: {path}\")\n",
        "        return self._iter()\n"
      ],
      "metadata": {
        "id": "gnypjoQvXBqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from river import evaluate, metrics, tree, ensemble, forest\n",
        "from river.datasets import synth\n",
        "from copy import deepcopy\n",
        "\n",
        "# -------------------------------\n",
        "# Define Stream Generators\n",
        "# -------------------------------\n",
        "binary_classification_datasets = [\n",
        "    synth.Agrawal(),\n",
        "    synth.AnomalySine(),\n",
        "    synth.ConceptDriftStream(),\n",
        "    synth.Hyperplane(),\n",
        "    synth.Mixed(),\n",
        "    synth.SEA(),\n",
        "    synth.STAGGER(),\n",
        "    synth.Sine()\n",
        "]\n",
        "\n",
        "multi_class_classification_datasets = [\n",
        "    synth.LED(),\n",
        "    synth.LEDDrift(),\n",
        "    synth.RandomRBF(),\n",
        "    synth.RandomRBFDrift(),\n",
        "    synth.RandomTree(),\n",
        "    synth.Waveform()\n",
        "]\n",
        "\n",
        "all_datasets = {\n",
        "    ds.__class__.__name__: ds for ds in binary_classification_datasets + multi_class_classification_datasets\n",
        "}\n",
        "\n",
        "# -------------------------------\n",
        "# Evaluation Loop\n",
        "# -------------------------------\n",
        "for dataset_name, stream in all_datasets.items():\n",
        "    print(f\"\\nğŸ” Dataset: {dataset_name}\")\n",
        "\n",
        "    # Re-initialize stream (take 1999 samples)\n",
        "    dataset = deepcopy(stream).take(1999)\n",
        "\n",
        "    # Define base model\n",
        "    base_model = tree.HoeffdingTreeClassifier(grace_period=50, delta=0.01)\n",
        "\n",
        "    # Define models\n",
        "    models = {\n",
        "        \"Static SRPClassifier\": ensemble.SRPClassifier(\n",
        "            model=base_model.clone(), n_models=10, seed=42\n",
        "        ),\n",
        "        \"ARFClassifier\": forest.ARFClassifier(\n",
        "            seed=8, leaf_prediction=\"mc\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Evaluate each model\n",
        "    for model_name, model in models.items():\n",
        "        dataset = deepcopy(stream).take(1999)  # Reset stream again\n",
        "\n",
        "        # Define metrics individually\n",
        "        acc = metrics.Accuracy()\n",
        "        prec = metrics.Precision()\n",
        "        rec = metrics.Recall()\n",
        "        kappa = metrics.CohenKappa()\n",
        "\n",
        "        # Combine for evaluation\n",
        "        metric_set = metrics.ClassificationReport([acc, prec, rec, kappa])\n",
        "\n",
        "        # Perform progressive validation\n",
        "        result = evaluate.progressive_val_score(\n",
        "            dataset=dataset,\n",
        "            model=model,\n",
        "            metric=metric_set,\n",
        "            print_every=0\n",
        "        )\n",
        "\n",
        "        # Output each metric manually\n",
        "        print(f\"{model_name:>25}:\")\n",
        "        for m in [acc, prec, rec, kappa]:\n",
        "            print(f\"  {m.__class__.__name__:>15}: {m.get():.4f}\")\n"
      ],
      "metadata": {
        "id": "G3msZRq3Xfc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}